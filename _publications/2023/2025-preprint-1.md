---
title:          "Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises"
date:           2025-09-19 00:00:00 +0800
selected:       false
# pub:            "The IEEE International Conference on Data Mining (ICDM)"
pub_date:       "2025"
pub_post:       'Preprint.'
# semantic_scholar_id: 11ac0b5634a282f1a0da204b98e7473d8b480dfb  # use this to retrieve citation count
abstract: >-
  Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.
# cover:          /assets/images/covers/cover2.jpg
authors:
  - <strong> Xinwen Zhang</strong>
  - Yihan Zhang
  - Hongchang Gao
links:
   Paper: https://arxiv.org/abs/2509.15543
  # Code: https://github.com
  # Unsplash: https://unsplash.com/photos/orange-fruit-on-white-table-cloth-ISX_imp8t1o
---
